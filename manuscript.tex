\documentclass[11pt]{article}

%==============Packages & Commands==============
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{tikz}
%%%<
\usepackage{verbatim}
%\usepackage[active,tightpage]{preview}
%\PreviewEnvironment{tikzpicture}
%\setlength\PreviewBorder{5pt}%

\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
% \geometry{a4paper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activat\usetikzlibrary{arrows}e for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{arrows}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage[longnamesfirst]{natbib} % For references
\bibpunct{(}{)}{;}{a}{}{,} % Reference punctuation
\usepackage{changepage}
\usepackage{setspace}
\usepackage{booktabs} % For tables
\usepackage{rotating} % For sideways tables/figures
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{dcolumn}
\usepackage{comment}
%\usepackage{fullwidth}
\newcolumntype{d}[1]{D{.}{\cdot}{#1}}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{3}{D{.}{.}{3}}
\newcolumntype{4}{D{.}{.}{4}}
\newcolumntype{5}{D{.}{.}{5}}
\usepackage{float}
\usepackage[hyphens]{url}
%\usepackage[margin = 1.25in]{geometry}
%\usepackage[nolists,figuresfirst]{endfloat} % Figures and tables at the end
\usepackage{subfig}
\captionsetup[subfloat]{position = top, font = normalsize} % For sub-figure captions
\usepackage{fancyhdr}
%\makeatletter
%\def\url@leostyle{%
%  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
%\makeatother
%% Now actually use the newly defined style.
\urlstyle{same}
\usepackage{times}
% \usepackage{mathptmx}
%\usepackage[colorlinks = true,
%						bookmarksopen = true,
%						pagebackref = true,
%						linkcolor = black,
%						citecolor = black,
% 					urlcolor = black]{hyperref}
%\usepackage[all]{hypcap}
%\urlstyle{same}
\newcommand{\fnote}[1]{\footnote{\normalsize{#1}}} % 12 pt, double spaced footnotes
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\def\citeaposs#1{\citeauthor{#1}' (\citeyear{#1})}
\newcommand{\bm}[1]{\boldsymbol{#1}} %makes bold math symbols easier
\newcommand{\R}{\textsf{R}\space} %R in textsf font
\newcommand{\netinf}{\texttt{NetInf}\space} %R in textsf font
\newcommand{\iid}{i.i.d} %shorthand for iid
\newcommand{\cites}{{\bf \textcolor{red}{CITES}}} %shorthand for iid
%\usepackage[compact]{titlesec}
%\titlespacing{\section}{0pt}{*0}{*0}
%\titlespacing{\subsection}{0pt}{*0}{*0}
%\titlespacing{\subsubsection}{0pt}{*0}{*0}
%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\setlength{\bibsep}{2pt}
%\renewcommand{\headrulewidth}{0pt}

%\renewcommand{\figureplace}{ % This places [Insert Table X here] and [Insert Figure Y here] in the text
%\begin{center}
%[Insert \figurename~\thepostfig\ here]
%\end{center}}
%\renewcommand{\tableplace}{%
%\begin{center}
%[Insert \tablename~\theposttbl\ here]
%\end{center}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Y}{\bm{\mathcal{Y}}}
\newcommand{\bZ}{\bm{Z}}

\usepackage[colorlinks = TRUE, urlcolor = black, linkcolor = black, citecolor = black, pdfstartview = FitV]{hyperref}


%============Article Title, Authors==================
\title{\vspace{-2cm} Inference on the Effects of Observed Features 
\\ in Latent Space Models for Networks } 


\author{ Zachary Jones \and Matthew Denny \and Bruce Desmarais \and Hanna Wallach} \date{\today}



%===================Startup=======================
\begin{document}
\maketitle



%=============Abstract & Keywords==================

\begin{abstract}

\noindent The latent space model (LSM) for network data is a generative probabilistic model that combines a generalized linear model with a latent spatial embedding of the network. It has been used to decrease error in the estimation of and inference regarding the effects of observed covariates. In applications of the LSM, it is assumed that the latent spatial embedding can control for unmeasured confounding structure that is related to the values of edges in the network. As far as we know, there has been no research that considers the LSM's performance in adjusting for unmeasured structure to reduce estimation and inferential errors. We investigate the LSM's performance via a Monte Carlo study. In the presence of an unmeasured covariate that can be appropriately modeled using a latent space, estimation and inferential error remain high under even moderate confounding. Additionally, when omitted network structure is not present the LSM induces additional bias and inferential error. However, the prediction error of the LSM when unmeasured network structure is present is substantially lower in most cases. We conclude that the LSM is most appropriately used for exploratory or predictive tasks.

\end{abstract}
\thispagestyle{empty}
% \doublespacing
% Description of the possible challenges
\section{Introduction}

Inferential analysis of political network data has grown increasingly sophisticated in recent years. Political networks scholars are well versed in the risks associated with ignoring unmodeled network structure. Dependencies such as reciprocity, transitivity, and homophily---if not accounted for---can lead to biased estimates and errors in hypothesis testing, much in the way that omitted variable bias can affect results in conventional regression models \citep{ward2007disputes,kinne2014,cranmerisq}. A number of statistical modeling frameworks have been proposed to account for confounding structure in network data that cannot be modeled with observed covariates. These include the exponential random graph model (ERGM) \citep[e.g., ][]{lazer2010,cranmer2011pa,desmarais2012psj}, the latent space model (LSM) \citep[e.g., ][]{ward2007disputes,ward2007persistent,kirkland2012multimember}, and the stochastic actor oriented model (SAOM) \citep[e.g., ][]{berardo2010ajps,kinne2014}. 

Despite their growing popularity, few studies exist that investigate the performance of these models in adjusting for confounders that can be represented as network structure.  The approach to adjusting for dependencies in the ERGM and SAOM is quite similar to adjusting for confounding covariates in regression modeling. The researcher specifies a set of dependencies that (s)he hypothesizes to be important in the generative model for the network. These dependencies are then explicitly included in a model that simultaneously represents the effects of observed covariates \citep{cranmer2011pa}. The LSM takes a different approach, which involves the incorporation of latent variables to model network structure. The LSM has the advantage over ERGM and SAOM in that researchers need not develop a set of hypothesized dependencies in order to model network structure that is not reflected in observed covariates. In the current study, we focus on the LSM, examining its performance in reducing estimation and inferential errors regarding the effects of observed covariates, via adjustment for confounding network structure.

\subsection{Central Problem}

Latent variable inference, generally conceived, presents the possibility of representing unmeasured data in statistical models. The LSM, introduced by \citet{hoff2002latent}, is used to estimate the effect of covariates in the presence of latent network structure. Here the distance function $|z_i - z_j|$ represents latent network structure as homophily with respect to latent variables.  The distance function is additively combined with a regression on observed dyadic covariates, $x_{ij}$, to form a linear predictor for tie prediction.  As with a GLM, a link function, $g^{-1}$, maps this linear predictor to the appropriate edge distribution.

$$\mathbb{E}(y_{ij} | x_{ij}) = g^{-1}(\alpha + \beta x_{ij} - |z_i - z_j|)$$

However, it is unclear that the introduction of a latent space decreases the expected error for the parameter(s) of interest when aspects of the network structure (e.g., homophily) that are unmeasured, are correlated with measured covariates. There are two reasons that using the LSM may lead to increased error. First, the latent configurations inferred may result in a representation of the network wherein a node's position in the latent space is spurriously correlated with the observed covariates, leading to reduced efficiency. Second, if the unobserved (i.e., latent) network structure is truly correlated with the observed covariates, the unobserved structure that can be correlated with the observed variable may be attributed to the observed variable, while the latent space parameters are used to model other sources of variation. 

We develop a simulation design in which we control the degree of collinearity between omitted network structure and observed covariates. We find that the LSM does not substantially reduce bias and inferential error with respect to the effects of observed covariates, in comparison to a generalized linear model (GLM). Additionally, the LSM may increase bias and inferential error when no omitted network structure is present, though we find that this effect may be reduced by appropriately scaling the prior of the latent positions. However, we find that the LSM substantially outperforms a GLM in terms of prediction. We conclude that the LSM should not be used to adjust for unmeasured confounders and is best used for exploratory and predictive tasks.

\section{Applications and Development of the Latent Space Model}

The LSM has seen use in a variety of fields in which network data is common, particularly the social sciences. The apparent appeal of the LSM appears to be driven primarily by the LSM's usefulness in modeling transitivity and homophily which are ubiquitous in social networks. In political science the LSM and variations on the form developed in \cite{hoff2002latent} have been used to estimate the effect of democracy on the probability of a militarized interstate dispute \citep{ward2007disputes}, the amount of portfolio investment between states \citep{cao2013democracies}, and the effect of multimember districts on the probability of collaboration between state legislators in the United States \citep{kirkland2012multimember}. Variations of the LSM developed for networks measured over time have been applied to the study of international trade, wherein the effects of various features of trading partners are estimated \citep{ward2013gravity}. In ecology the LSM has been used to study the sociality of elephants \citep{vance2009social} and orcas \citep{fearnbach2014spatial}, to discover ecological communities \citep{fletcher2011social, fletcher2013network}, and to study food webs \citep{chiu2011unifying}. In epidemeology it has been used to identify clusters of infected persons for later isolation \citep{zhang2015cluster} and to study patterns of interaction amongst physicians \citep{paul2014results}. In marketing and business research it has been used to study inter-group trust \citep{dass2011impact}, optimal bundling and pricing of goods and brands for retailers \citep{dass2012assessing}. Lastly, in neuroscience it has been proposed as a method for modelling fMRI data \citep{simpson2013analyzing}.

% need to add marketing stuff and probably should look again for applications
% also need to read and fit in the newer ward student stuff

Although the LSM is arguably most useful as an exploratory or predictive model (see \citet{shmueli2010explain} for a discussion of the differences between predictive and explanatory modeling), it has been applied in some cases to reduce estimation and/or inferential error with respect to the effects of measured covariates. If the LSM does reduce inferential/estimation error under certain conditions, it will serve as a valuable general model for explanatory analysis of network data, especially since its use does not require the researcher to specify a set of network dependencies from theory. However, as far as we know, there has been no research into the performance of the LSM in adjusting for confounding network structure. Additionally, the predictive performance of the LSM has only seen limited evaluation \citep{hoff2002latent}, despite having been used for model selection \citep{ward2013gravity, fletcher2011social, fletcher2013network, chiu2011unifying}. Hence, finding the conditions under which the LSM reduces prediction error also may affect future use of the LSM.

The LSM has seen a substantial amount of subsequent development and extension. The latent space has been represented as a $k$-dimensional Euclildean space and by latent factors \citep{hoff2002latent, hoff2009multiplicative}. Additional structure has been introduced by adding random effects, which, for example, may involve sender or receiver specific effects for directed networks which capture differential activity rates amongst nodes \citep{hoff2003random}. Within this framework \cite{westveld2011mixed} models dynamic network data by treating the latent space as a stochastic process. \cite{handcock2007model} enable the LSM to model clustering that is not representable as homophily (i.e., stochastic equivalence) by combining the LSM with latent cluster models. \cite{hoff2008modeling} shows that the LSM and latent cluster models are special cases of an ``eigenmodel.'' That is, an eigendecomposition of a symmetric sociomatrix can be used to represent both latent space and cluster models, but not vice-versa.

\section{Research Design}



\subsection{Priors for the Latent Space Model}

\subsubsection{Current Implementations}

What priors are currently used and why? \cite{hoff2002latent} propose the use of diffuse independent normal priors for the latent positions and regression parameters. They also recognize the fact that isolates' positions are weakly identified in the LSM, and in one application they actually just exclude the isolate.\footnote{Excluding isolates seems fine if the inferential purpose is to estimate positions in the latent space. However, such deletion would introduce bias in estimating covariate effects.} \cite{hoff2002latent} experiment with the use of an exponential prior for the intercept, which they argue sets a lower bound on the probability of a tie between nodes. Of course, the latent space positions could simply drift further apart in order to compensate for the high intercept. The following paragraph on priors appears on page 1097.

\begin{quote}
We have not discussed in detail the choice of a prior distribution for latent positions in this article. Although simple, the diffuse independent normal priors presented in the examples may not accurately represent prior beliefs about the structure of social networks. More appropriate might be clustered point processes or mixtures of normals with an unknown number of components. Such priors could allow one to incorporate prior information on tendencies for clustering, without specifying cluster membership. This would add another level of hierarchy to the analysis, although the resulting model would be more flexible and perhaps more accurately represent any tendencies of populations to form segregating groups.
\end{quote}

\citep{Krivitsky2009} implement a hierarchical model in which each node's latent position is drawn from a mixture of $G$ normals. The covariate priors are independent normals.

\subsubsection{Alternatives to consider}

Would regularizing/sparsity-inducing priors help with identifying the covariate effects? \cite{mitchell1988} propose a ``spike and slab'' prior as a class of priors for the coefficients in linear regression. These priors are well-suited to the problem of variable selection for linear regression. The prior is formed as a mixture between a diffuse uniform prior and a point mass at zero. Such a prior would not be appropriate for latent space coordinates since weakly identified coordinates of isolates or separate components would likely exhibit strange behavior at the boundary of the diffuse uniform. We might consider instead mixing, e.g., a point mass at zero with a distribution such as the Cauchy, which is fairly flat in the tails, but is defined on the entire real line and still exhibits a surface that descends from the center of the distribution. This prior structure would build a preference into the model for using the observed covariates, rather than the latent coordinate parameters, to explain tie formation. This might reduce any bias that the LSM could introduce by preferentially using the latent positions to model variation due to the observed covariates.

% What priors could be used to avoid collinearity between observed covariates and latent space? Look to Bayesian Latent Trait models and specifically differential item functioning (DIF) in the IRT literature.

Rather than using a mixture of distributions for the prior, we might instead use an informative prior. Using informative priors assures that the latent space coordinates are weakly identified, meaning that at some distance from the origin the increase in the likelihood from moving isolates away from the other nodes, or components away from each other, is offset by decreases in the prior from moving coordinates away from the origin. However, we may also use the priors to penalize the model for inferring latent coordinates that result in distances that are comparable in scale to the observed covariates. This would embed a tendency for the model to use the observed covariates, rather than the latent space, to model the outcome. Consider the following example of a one-dimensional spatial model with an additional dyadic covariate. 

$$\mathbb{P}(y_{ij} = 1) = \text{logit}^{-1}(\alpha + \beta x_{ij} -  |z_i - z_j|)$$

If we assume that the $\mathbf{z}$ are independent and distributed $\mathcal{N}(0,\sigma^2)$, it is straightforward to derive the distribution of $|z_i - z_j|$.\footnote{The assumption that $\mathbf{z} \sim \mathcal{N}(0,\sigma^2$) is consistent with $\mathbf{z}$ being drawn from the prior that is conventionally used in the LSM.} Since it is a special case of a ``normal difference distribution'', we know that $z_i - z_j \sim \mathcal{N}(0,2\sigma^2)$ \citep{devore2012}. The normality of $z_i - z_j$ implies that $|z_i - z_j|$ has a folded normal distribution \citep{leone1961}. Following from this result, we know that the variance of  $|z_i - z_j|$ is $$\sigma^2(2-4/\pi). $$ Let $\theta^2$ be the empirical variance of $\mathbf{x}$. Then the variance in the linear predictor is $\beta^2\theta^2$. Given an estimate $\hat{\beta}$, we can set the variance of the latent space prior to approximately $\hat{\beta}^2\theta^2/(2-4/\pi)$ in order to tune the prior to avoid replicating $\mathbf{x}$ via the distances between latent coordinates. This result is limited in that it requires the assumption of normally distributed latent coordinates and applies only to a single latent dimension. However, it may provide a reasonable approximation in the case of non-normal coordinates and/or multiple dimensions. \cite{thirey2015} derive the distribution of Euclidean Distances between two $k$-dimensional points where each coordinate is drawn from a standard normal distribution.

\section{Analysis}

\subsection{Simulation Design}

We study how the LSM behaves in the situation in which we have some observed covariates as well as omitted network structure which can be represented using a Euclidean latent space. For simplicity we consider a unidimensional latent space. To generate an observed covariate which has a controllable collinearity with the latent network structure we follow a three-step process. First, we simulate unidimensional positions for each node, drawing from a normal distribution, and calculate the Euclidean distance $\mathbf{d}$ between each pair of positions. Second, given a target covariance matrix ($\Sigma$) among the covariates and distances, $\langle \mathbf{x}, \mathbf{d} \rangle$, we derive the conditional mean vector and covariance, assuming that $\mathbf{x}$ has a normal distribution given $\mathbf{d}$ (see \cite[pp. 116--117]{eaton1983} for the conditional normal derivation). Third, we simulate $x$ as a normal random variable with the respective conditional means and covariance. Finally we standardize $x$ to have zero mean and unit variance. To generate the covariance matrix $\Sigma$ which controls the dependence between the omitted network structure and the observed covariate $\mathbf{x}$ we utilize the C-vine method of \cite{lewandowski2009generating}. % fill in a basic description of the method here

We consider three exponential family distributions for the edges: Gaussian, binomial, and poisson. Additionally we control the number of nodes in the network $n = 25, 50, 100$, fix the dimension of the latent space at 1 for simplicity, and consider three values of the collinearity parameter $\eta$, $1$, $100$, and $1000000$, where $1$ corresponds to a uniform distribution over $[0, 1]$, moderate collinearity, and $1,000,000$ to independence. See Figure \ref{fig:vine} for the distribution of the absolute value of the maximal off-diagnoal correlation generated at each value of $\eta$.

\begin{figure}
\includegraphics[width=\textwidth]{max_r_vine.png}
\caption{The distribution of the absolute value of the maximum off-diagonal correlation. \label{fig:vine}}
\end{figure}

We consider the LSM with several different priors on the coefficient for the observed covariate and the latent space. We set the prior variance of $\beta$ to be either $1$ or $10$ and alternatively use a diffuse normal prior on the latent space or scale it by $\hat{\beta}^2\theta^2/(2-4/\pi)$, where $\theta^2$ is the empirical variance of $x$.

The LSM is estimated using the canonical implementation in the \texttt{latentnet} package in \texttt{R}. An initial run of 10,000 burn in iterations, followed by 1,000,000 iterations of the sampler. Every 100th iteration is saved. Convergence in the log probability of the model is assessed using the Geweke diagnostic. If the convergence criteria is satisfied the simulation continues to the next set of arguments, otherwise the number of iterations is doubled. If the convergence criteria is still not satisfied, then the aforementioned step in the simulation is flagged for review. At each point in the simulation's parameter space, we execute 1,000 Monte Carlo iterations.

One of the primary difficulties in executing the above simulation design is the computational cost of estimating the LSM. We utilize the \texttt{BatchExperiments} \R package to construct and execute our computational experiments on a Torque cluster \cite{bischl2015batchjobs}.

We compute the MLE estimated by iteratively reweighted least squares in \texttt{R}. For the LSM we use the posterior mode as our point estimate. In the cases where edges are binomial and there is omitted network structure, we scale the estiamtes using the reciprocal of the bias of $\beta$ when $\mathbf{x}$ and $\mathbf{d}$ are independent: $\sqrt{\frac{3.28 + \beta^2 \text{Var}(\mathbf{d})}{3.29}}$, where $3.29$ is the variance of a standard logistic distribution (See the derivation of the bias under an independent but ommitted covariate in \cite{mood2010logistic}).

We consider networks of varying size (i.e. number of nodes) and collinearity structure between the ommitted distance between nodes and the measured covariates. For each point in this space we evaluate the mean square prediction error of the model on new edges drawn condtional on $\mathbf{x}$ and $\mathbf{d}$, the bias of the estimated coefficient for the measured covariate, and the Type-1 and 2 error rates. Inference regarding Credible intervals for $\beta$ are defined by using the region of highest posterior density which covers $95\%$ of the marginal posterior distribution.

To evaluate estimation error we compute the bias. Figures \ref{fig:estimation_ls} and \ref{fig:estimation_nls} shows the results. In \ref{fig:esimation_ls} we can see that with a uniform distribution over the correlation between the distances and the measured covariate, all methods (except the true model wherein the distances are treated as observed )show substantial bias. In most cases the LSM performs better than the GLM, sometimes by as much as $50\%$, however, in absolute terms the amount of bias is large. For both the GLM and the LSM bias decays rapidly as the degree of correlation between the distances and the measured covariate decreases. In \ref{fig:estimation_nls} we can see that when there is no ommitted network structure the LSM exhibits bias greater than that of the GLM, though it does appear that scaling the prior of the latent positions to match the scale of the measured covariate decreases this tendency. 

\begin{figure}
\includegraphics[width=\textwidth]{estimation_ls.png}
\caption{The bias of estimates of the effect of the observed covariate $\mathbf{x}$ when there is an ommitted variable. The $x$-axis gives the value of the parameter $\eta$ which controls the degree of dependence between $\mathbf{x}$ and ommitted covariate. Lower values of $\eta$ indicate higher levels of dependence between the observed and ommitted covariate. The $y$-axis gives a Monte Carlo estimate of the bias. The number of nodes are indicated in the top panels, while the distributional family of the edges is shown on the right panel. Each panel represents 4 values of $\eta$ with 1,000 Monte Carlo iterations executed at each point.
\label{fig:estimation_ls}}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{estimation_nls.png}
\caption{The bias of estimates of the effect of the observed covariate $X$ when there is no ommitted variable.
\label{fig:estimation_nls}}
\end{figure}

Figures \ref{fig:inference_type_1} and \ref{fig:inference_type_2} show the inferential error rates of the LSM with different priors for $\beta$ and the latent positions, as well as a GLM. Type-1 error rates shown in \ref{fig:inference_type_1} for the LSM are in general lower than those of a GLM when an unmeasured covariate exists. Under uniform correlation between the omitted network structure and the observed covariate the Type-1 error rate is still often more than 10 times greater than the nominal rate of .05. Again, scaling the prior of the latent space to match that of the observed covariate appears to make the LSM's error rate comparable to that of the GLM. Type-2 error rates, shown in \ref{fig:inference_type_2} are also substantially above their nominal rate when there is a uniform distribution on the strength of confounding, though the inflation is not as bad as that of the Type-1 rates.

\begin{figure}
\includegraphics[width=\textwidth]{inference_type_1.png}
\caption{Monte Carlo estimates of the Type-1 error regarding the effect of the observed covariate $\mathbf{x}$ are shown on the $y$-axis. Here, $\beta = 0$ and the error rate shown in each panel in that row gives 1 minus the probability of a 95\% confidence region (for the LSM) or interval (for the GLM) including $0$, giving the probability that a true null hypothesis of $\beta = 0$ is falsely rejected. \label{fig:inference_type_1}}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{inference_type_2.png}
\caption{Monte Carlo estimates of the Type-2 error regarding the effect of the observed covariate $\mathbf{x}$ are shown on the $y-axis$. Here, $\beta = 1$, and the error rate shown in each panel in each row gives the probability of the probability/confidence intervals covering $0$, giving the probability of accepting a false null hypothesis. \label{fig:inference_type_2}}
\end{figure}

Generalization error, in this case defined as the expected prediction error on new edges generated using a fixed set of latent positions for the nodes and a fixed observed covariate is shown in figure \ref{fig:generalization}. In nearly all cases the LSM substantially outperforms the GLM, especially when the ommitted covariate is not highly collinear with the observed covariate. In nearly all cases it performs as well as the true model.

\begin{figure}
\includegraphics[width=\textwidth]{generalization.png}
\caption{The $x$-axis shows a Monte Carlo estimate of the mean square error for edge values drawn from the appropriate distribution with the observed covariate $\mathbf{x}$ and the latent positions $\mathbf{z}$ fixed. In the binomial case the Brier score is computed and in the poisson and normal cases the mean-square-error. \label{fig:generalization}}
\end{figure}

\section{Conclusion}

We conclude that the primary reason for using the LSM or one of its many variants, rather than a GLM, should be interest in using the latent space to uncover patterns of interest or the prediction of edges in networks where there is likely to be omitted structure that is not adequately modelled using measured covariates. Although inferential and estimation errors are somewhat smaller than that of a GLM when there is omitted network structure, even moderate collinearity between the omitted structure and the measured covariate leads to substantial bias and inferential error. The LSM cannot control for unmeasured confounders. Additionally, if such network structure does not exist, the LSM may induce additional bias and inferential error, though this effect can be moderated by scaling the prior on the latent positions to match that of the measured covariates.

\newpage

\bibliographystyle{apsr}
\bibliography{ref}

\end{document}